---
title: "Predicting quality of white wine from various characteristics"
author: "DSCI 522 group 27"
bibliography: references_white_wine.bib
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(feather)
library(arrow)
library(knitr)

path <- "../results/tuned_crossval_results.feather"
df <- read_feather(path)
```

# **Summary**

Here we tried different models such as dummy regressor, ridge and random forest regressor to predict the white wine quality. When we carried out the cross-validation for these three models, we chose random forest regressor as our best performance model by comparing different metrics. We tried hyperparameter optimization with random forest regressor to get the r2 score of 0.492 as our final test score with a negative mean absolute error of -0.443, which seems to be not reasonable here (Note that we have imbalanced data). Therefore, random forest regressor may not be an appropriate model to use here. However, we can find other complex models to improve our test scores, or we can carry out a different metric or tune other hyperparameters to get a better result. Moreover, we can also change the prediction task from a regression problem to a classification problem in order to find a better prediction.

# **Introduction**

The wine market occupies a significant position among consumers. For manufacturers, the quality of alcohol significantly affects the sales of alcoholic beverages, but the taster is not necessarily the only standard for judging the quality of alcoholic beverages. We can establish a model to estimate the quality of alcoholic drinks through chemical substances. However, this may require a lot of professional knowledge. We found a good article which was written by Dr. P. Cortez, Dr. A. Cerdeira, Dr. F. Almeida, Dr. T. Matos and Dr. J. Reis, and they used a data mining approach to get promising results comparing neural network methods [@CORTEZ2009547].

Here we want to try different regression models to predict the wine quality based on the physicochemical test features. Answering this question is crucial since we want to support the wine tasting evaluations of oenologists and contribute to wine production [@CORTEZ2009547].

# **Methods**

## **Data**

The dataset that we used came from the University of California Irvine (UCI) machine learning repository and was collected by Paulo Cortez, University of Minho, GuimarÃ£es, Portugal and A. Cerdeira, F. Almeida, T. Matos with help from J. Reis, Viticulture Commission of the Vinho Verde Region(CVRVV), Porto, Portugal in 2009. The dataset contains the results of various physiochemical tests on white "Vinho Verde" wine samples from Northern Portugal and can be found [here](https://archive.ics.uci.edu/ml/datasets/wine+quality) specifically with the [white wine dataset](%5Bhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv). No additional features or specific branding of each wine is available in the dataset for privacy purposes. Each row in the dataset represents a single wine which was tested and scored based on sensory data.

## **Analysis**

A classification model was built with python scripts using the sk-learn `RandomForestRegressor` algorithm and allowed us to predict a sensory score based on the physiochemical testing information recorded for each wine[@Python], [@scikit-learn]. Because of the privacy constraints of the data our dataset is somewhat limited since useful potentially factors that might influence the scoring such as grape types, brand names, or price are not available to us. Assumptions we made regarding this dataset are that the quality scores came from the opinions of wine critics and that testing for all wines was consistent. The model was fit using all of the variables from the dataset. Hyperparameters `n_estimators` and `max_depth` were optimized via random search while all other hyperparameters used the default sklearn `RandomForestRegressor` values. The data was processed using the pandas package and EDA was performed using the pandas-profiling package [@reback2020pandas] [@pandasprofiling2019]. This document was compiled using an R document file with scripts run using the docopt package [@R], [@docopt]. Tables were stored using feather files (with dependency on arrow) and displayed using knitr's kable function [@feather], [@arrow], [@knitr]. This document was compiled using rmarkdown [@rmarkdown].

# **Results & Discussion**

After splitting our dataset into a training set and a validation set we plotted the distribution of the quality scores for each wine (Figure 1). Despite the quality scoring being performed a scale from 1-10 only values in the range of 3-9 were observed. 6 was the most common score observed across all testing.

```{r fig_1, echo=FALSE, fig.cap="Figure 1. Quality distribution of wines in the training and test datasets.", out.width = '60%'}
knitr::include_graphics("../results/quality_distributions_figure.png")
# [picture of our correlation plot]
```

In order to determine which model works best with our data we decided to test both the `RidgeCV` and `RandomForestRegressor` to compare them against the dummy regressor model. We present the cross-validation values of this testing in Table 1. We determined that random forest methods provided the best training and validation model scores and decided to proceed with those.

```{r table_1, echo=FALSE, out.width = '60%'}
path <- "../results/initial_crossval_results.feather"
kable(read_feather(path),
      caption="Table 1. Table of cross-validation results for each tested model")
```

We found that a random forest classifier worked best with our dataset and decided perform random search hyperparameter optimization to tune the hyperparameters `n_estimators` and `max_depth`, which we determined produced the best scoring model with the values of 300 and 10 respectively. Running a `RandomForestRegressor` with these hyperparameters resulted in a training r2 score of 0.929 and a validation r2 score of 0.505 (Table 2).

```{r table_2, echo=FALSE, out.width = '60%'}
path <- "../results/tuned_crossval_results.feather"
kable(read_feather(path),
      caption="Table 2. Table of cross-validation results of the tuned random forest model")
```

Running our hyperparamter tuned `RandomForestClassifier` model on our test data resulted in an r2 test score of 0.492 and a negative mean absolute error of -0.443 (Table 3). These results are comparable to those that we observed in our validation scoring, which produced similar values (with scoring differing by only about 0.01).

```{r table_3, echo=FALSE, out.width = '60%'}
path <- "../results/tuned_test_results.feather"
kable(read_feather(path),
      caption="Table 3. Tuned test results of RandomForestClassifier.")
```

We then examined the weight of the features present in our best scoring `RandomForestClassifier` and charted the weight of each in the model (Figure 2). Alcohol was found to be the feature most heavily associated with higher wine quality scores with a target weight of 0.24. Other features such as density, citric acid, and sulphates appear to have limited weight in our model. In an attempt to further improve the scoring of our model we decided to cut all features with a target weight lower than 0.10, meaning we decided to run a model that predicted quality scores based on the features alcohol, free sulfur dioxide, and volatile acidity.

```{r fig_2, echo=FALSE, fig.cap="Figure 2. Bar chart showing the target weights of different features of our RandomForestRegressor model.", out.width = '60%'}
knitr::include_graphics("../results/weights_figure.png")
```

# Limitations & Future

Some potential limitations of our model are that we have only tested a handful of different regression methods and only have performed light hyperparameter optimization via a random search. There likely exists combinations of models and hyperparamters (perhaps determined through a grid search, though this would increase the runtime of our model significantly) which would lead to better scoring in our model. For example, using support vector machine (SVM) methods might be a more effective way to predict wine scores as they were specifically mentioned by Cortez et al. in their paper analyzing the dataset [@CORTEZ2009547]. Another way to improve our model would be to implement a form of feature selection (such as RFECV) given that we our current method involves us manually selecting our features based on their target weights. Another way to improve this model would be to work with a larger dataset (i.e. with wine/judges from around the world) or with a greater number of features since the one we are currently working with does not list some information that could potentially be correlated with scoring (type of grape used in the wine, price, etc.) which are currently omitted for the sake of privacy protection.

# References
